{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON'T CHANGE this part: import libraries\n",
    "import numpy as np\n",
    "import scipy\n",
    "import json\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "import re\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON'T CHANGE this part: read data path\n",
    "train_set_path, valid_set_path, random_number = input().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# 1. preprocess: converting text to lowercase, coverting number, tokenization, removing stopword, stemming\n",
    "# 2. embedding: hitogram matrix\n",
    "# 3. classifier using linear regression\n",
    "# 4. accuracy (for metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Rate init with input\n",
    "class Rate:\n",
    "  def __init__(self, train_set_path, valid_set_path, random_number):\n",
    "    # Store input\n",
    "    self.train_set_path = train_set_path\n",
    "    self.valid_set_path = valid_set_path\n",
    "    self.random_number = int(random_number)\n",
    "    self.valid_at_random_number = []\n",
    "    self.train_vocab = []\n",
    "\n",
    "    # Get A, B of train and valid data (after preprocess and embedding json data)\n",
    "    self.train_A, self.train_B = self.processing_input(is_training=True)\n",
    "    self.valid_A, self.valid_B = self.processing_input(is_training=False)\n",
    "\n",
    "    # Calculate x_hat (classifier using linear regression)\n",
    "    self.x_hat = self.classifier()\n",
    "\n",
    "    # Calculate M2 accuracy\n",
    "    self.m2_accuracy = self.accuracy()\n",
    "\n",
    "  # 1. Preprocess\n",
    "  def preprocess(self, text, is_training):\n",
    "    # Converting text to lowercase by lower()\n",
    "    text = text.lower()\n",
    "    # Converting number to 'num' word using RegEx, number is determined by r'[0-9]+', replace using re.sub()\n",
    "    text = re.sub(r'[0-9]+', 'num', text)\n",
    "    # Tokenization using word_tokenize()\n",
    "    tokens = word_tokenize(text)\n",
    "    # Removeing stopword by check w in stopwords.words() or not\n",
    "    tokens = [w for w in tokens if not w in stopwords.words()]\n",
    "    # Stemming by PorterStemmer().stem() function\n",
    "    ps = PorterStemmer()\n",
    "    stemming = [ps.stem(w) for w in tokens]\n",
    "    # Convert to 'unk' when not training (validating)\n",
    "    if not is_training:\n",
    "      # If word not in train_vocab, then convert it to 'unk'\n",
    "      stemming = [w if w in self.train_vocab else 'unk' for w in stemming]\n",
    "    # Return text preprocessed\n",
    "    return stemming\n",
    "\n",
    "  # 2. Embedding\n",
    "  def embedding(self, docs, is_training):\n",
    "    # Create unique vocab from training input if training\n",
    "    if is_training:\n",
    "      unique = set()\n",
    "      for doc in docs:\n",
    "        for word in doc:\n",
    "          unique.add(word)\n",
    "      unique = list(unique)\n",
    "      # Add 'unk' word to unique if it's not there, to handle 'unk' word from valid if valid's word not in train_vocab\n",
    "      if 'unk' not in unique:\n",
    "        unique.append('unk')\n",
    "      # Copy unique list to train_vocab to handle data when validating\n",
    "      self.train_vocab = unique[:]\n",
    "    # If not training (validating), then unique also is train_vocab (unique vocab from training input we created)\n",
    "    if not is_training:\n",
    "      unique = self.train_vocab\n",
    "    # Return Word count vector for all document in input\n",
    "    return np.array([[doc.count(word) for word in unique] for doc in docs])\n",
    "\n",
    "  # 3. Classifier\n",
    "  def classifier(self):\n",
    "    # Return x_hat from train_A and train_B\n",
    "    return np.linalg.pinv(self.train_A) @ self.train_B\n",
    "\n",
    "  # 4. Accuracy\n",
    "  def accuracy(self):\n",
    "    # vB is label of valid data, got from calculate softmax of valid_B on row (axis=1), then return argmax (index of max softmax) on  row (axis=1) then plus 1 (since index 0 is rate 1, index 1 is rate 2,...)\n",
    "    vB = np.argmax(scipy.special.softmax(self.valid_B, axis=1), axis=1) + 1\n",
    "    # cB is label of valid data, got from formula valid_A @ x_hat, then using softmax and argmax as above\n",
    "    cB = np.argmax(scipy.special.softmax(self.valid_A @ self.x_hat, axis=1), axis=1) + 1\n",
    "    # Evaluate accuracy base on formula sum(right) / total. sum(right) is sum of equal pairs between vB and cB, total is width of vB (or cB)\n",
    "    return np.sum(vB == cB) / vB.shape[0]\n",
    "\n",
    "  # Standardized lables to vector\n",
    "  def standardized(self, lables):\n",
    "    res = []\n",
    "    for label in lables:\n",
    "      # Create standard is zeros 5D vector (rate 1 to 5)\n",
    "      standard = [0, 0, 0, 0, 0]\n",
    "      # Set standard at index [label - 1] to 1\n",
    "      standard[int(label)-1] = 1\n",
    "      # Add standard to result\n",
    "      res.append(standard)\n",
    "    # Return numpy array of vector of standardized labels\n",
    "    return np.array(res)\n",
    "\n",
    "  def processing_input(self, is_training=False):\n",
    "    # Get path of input (train or valid) depend on is_training variable\n",
    "    path = self.train_set_path if is_training else self.valid_set_path\n",
    "    # Open file input from path\n",
    "    with open(path, 'r') as j:\n",
    "        # Load json data from file input\n",
    "        json_data = json.load(j)\n",
    "        # 1. Preprocess json data at 'reviewText' field\n",
    "        preprocessed_data = [self.preprocess(x['reviewText'], is_training) for x in json_data]\n",
    "        # If not training (validating), save preprocessed_data at random_number (from input) to valid_at_random_number to return to output\n",
    "        if not is_training:\n",
    "          self.valid_at_random_number = preprocessed_data[self.random_number]\n",
    "        # 2. Embedding preprocessed_data to histogram vector\n",
    "        embedded_data = self.embedding(preprocessed_data, is_training)\n",
    "        # Standarized labels ('overall' field) to vectors\n",
    "        standardized_label = self.standardized([x['overall'] for x in json_data])\n",
    "        # Return A (embedded_data after add 1 column of 1 to left most of it) and B (standardized_label)\n",
    "        return np.insert(embedded_data, 0, 1, axis=1), standardized_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['pen', 'never', 'get', 'old', '.', 'prefer', 'unk', 'point', '.', 'love', 'unk', 'color', '.']\nM2 - 0.7\n"
    }
   ],
   "source": [
    "rate = Rate(train_set_path, valid_set_path, random_number)\n",
    "print (rate.valid_at_random_number)\n",
    "print ('M2 - {}'.format(rate.m2_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}