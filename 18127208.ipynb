{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON'T CHANGE this part: import libraries\n",
    "import numpy as np\n",
    "import scipy\n",
    "import json\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON'T CHANGE this part: read data path\n",
    "train_set_path, valid_set_path, random_number = input().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# 1. preprocess: converting text to lowercase, coverting number, tokenization, removing stopword, stemming\n",
    "# 2. embedding: hitogram matrix\n",
    "# 3. classifier using linear regression\n",
    "# 4. accuracy (for metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Rate init with input, then process input, classifier and calculate accuracy of M2\n",
    "class Rate:\n",
    "  def __init__(self, train_set_path, valid_set_pangth, random_number):\n",
    "    # Store input\n",
    "    self.train_set_path = train_set_path\n",
    "    self.valid_set_path = valid_set_path\n",
    "    self.random_number = int(random_number)\n",
    "    self.valid_at_random_number = []\n",
    "    self.train_vocabs = []\n",
    "\n",
    "    # Get A, B of train and valid data (after preprocess and embedding json data)\n",
    "    self.processing_input(is_training=True)\n",
    "    self.train_A, self.train_B = self.processing_input(is_training=True)\n",
    "    self.valid_A, self.valid_B = self.processing_input(is_training=False)\n",
    "\n",
    "    # Calculate x_hat (classifier using linear regression)\n",
    "    self.x_hat = self.classifier()\n",
    "\n",
    "    # Calculate M2 accuracy\n",
    "    self.m2_accuracy = self.accuracy()\n",
    "\n",
    "  # 1. Preprocess\n",
    "  def preprocess(self, text, is_training):\n",
    "    # Converting text to lowercase by lower()\n",
    "    text = text.lower()\n",
    "    # Converting number to 'num' word using RegExr, number is determined by r'\\d+', replace using re.sub()\n",
    "    text = re.sub(r'\\d+', ' num ', text)\n",
    "    # Tokenization using word_tokenize()\n",
    "    tokens = word_tokenize(text)\n",
    "    # Removeing stopword by check whether w in stopwords.words() or not\n",
    "    tokens = [w for w in tokens if not w in stopwords.words(\"english\")]\n",
    "    # Stemming by PorterStemmer().stem() function\n",
    "    ps = PorterStemmer()\n",
    "    stemming = [ps.stem(w) for w in tokens]\n",
    "    # Convert to 'unk' when not training (validating)\n",
    "    if not is_training:\n",
    "      # If word not in train_vocab, then convert it to 'unk'\n",
    "      stemming = [w if w in self.train_vocabs else 'unk' for w in stemming]\n",
    "    # Return text preprocessed\n",
    "    return stemming\n",
    "\n",
    "  # Get train vocabs from preprocessed train data\n",
    "  def get_vocabs(self, preprocessed_data):\n",
    "    # Vocabs is vector of words in train data, with no duplicates by using set\n",
    "    vocabs = {'unk'}\n",
    "    for doc in preprocessed_data:\n",
    "      for word in doc:\n",
    "        vocabs.add(word)\n",
    "    # Convert to list\n",
    "    vocabs = list(vocabs)\n",
    "    return vocabs\n",
    "\n",
    "  # 2. Embedding\n",
    "  def embedding(self, docs):\n",
    "    # Return Word count vector for all document in input\n",
    "    return np.array([[doc.count(word) for word in self.train_vocabs] for doc in docs])\n",
    "\n",
    "  # 3. Classifier\n",
    "  def classifier(self):\n",
    "    # Return x_hat from train_A and train_B\n",
    "    return np.linalg.pinv(self.train_A) @ self.train_B\n",
    "\n",
    "  # 4. Accuracy\n",
    "  def accuracy(self):\n",
    "    # vB is label of valid data, got from calculate softmax of valid_B on row (axis=1), then return argmax (index of max softmax) on  row (axis=1) then plus 1 (since index 0 is rate 1, index 1 is rate 2,...)\n",
    "    vB = np.argmax(scipy.special.softmax(self.valid_B, axis=1), axis=1) + 1\n",
    "    # cB is label of valid data, got from formula valid_A @ x_hat, then using softmax and argmax as above\n",
    "    cB = np.argmax(scipy.special.softmax(self.valid_A @ self.x_hat, axis=1), axis=1) + 1\n",
    "    # Evaluate accuracy base on formula sum(right) / total. sum(right) is sum of equal pairs between vB and cB, total is width of vB (or cB)\n",
    "    return np.sum(vB == cB) / vB.shape[0]\n",
    "\n",
    "  # Standardized lables to vector\n",
    "  def standardized(self, lables):\n",
    "    res = []\n",
    "    for label in lables:\n",
    "      # Create standard is zeros 5D vector (rate 1 to 5)\n",
    "      standard = [0, 0, 0, 0, 0]\n",
    "      # Set standard at index [label - 1] to 1\n",
    "      standard[int(label)-1] = 1\n",
    "      # Add standard to result\n",
    "      res.append(standard)\n",
    "    # Return numpy array of vector of standardized labels\n",
    "    return np.array(res)\n",
    "\n",
    "  # Processing input: load data from path, preprocess data,\n",
    "  def processing_input(self, is_training=False):\n",
    "    # Get path of input (train or valid) depend on is_training variable\n",
    "    path = self.train_set_path if is_training else self.valid_set_path\n",
    "    # Open file input from path\n",
    "    with open(path, 'r') as j:\n",
    "        # Load json data from file input\n",
    "        json_data = json.load(j)\n",
    "        # 1. Preprocess json data at 'reviewText' field\n",
    "        preprocessed_data = [self.preprocess(x['reviewText'], is_training) for x in json_data]\n",
    "        # If training then get train_vocabs from preprocessed_data\n",
    "        if is_training:\n",
    "          self.train_vocabs = self.get_vocabs(preprocessed_data)\n",
    "        # If not training (validating), save preprocessed_data at random_number (from input) to valid_at_random_number to return to output\n",
    "        else:\n",
    "          self.valid_at_random_number = preprocessed_data[self.random_number]\n",
    "        # 2. Embedding preprocessed_data to histogram vector\n",
    "        embedded_data = self.embedding(preprocessed_data)\n",
    "        # Standarized labels ('overall' field) to vectors\n",
    "        standardized_label = self.standardized([x['overall'] for x in json_data])\n",
    "        # Return A (embedded_data after add 1 column of 1 to left most of it) and B (standardized_label)\n",
    "        return np.insert(embedded_data, 0, 1, axis=1), standardized_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['pen', 'never', 'get', 'old', '.', 'prefer', 'medium', 'point', '.', 'love', 'unk', 'color', '.']\nM2 - 0.512\n"
    }
   ],
   "source": [
    "# Create rate object from input\n",
    "rate = Rate(train_set_path, valid_set_path, random_number)\n",
    "# Print preprocessed data of valid at random number\n",
    "print (rate.valid_at_random_number)\n",
    "# Print accuracy of M2\n",
    "print ('M2 - {}'.format(rate.m2_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}